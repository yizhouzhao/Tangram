{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fold_actions import point_line_symmetry, matrix_line_symmetry, read_cloth_image, fold_action_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = \"data/fold/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothImages(Dataset):\n",
    "    def __init__(self, folder_path,  transforms=None, revert=False):\n",
    "        self.revert = revert\n",
    "        self.data_root_folder = folder_path\n",
    "        self.image_list, self.action_list  = self.load_image_list(self.data_root_folder)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def load_image_list(self, folder_path):\n",
    "        image_list = []\n",
    "        action_list = None\n",
    "        for dress_folder in os.listdir(folder_path):\n",
    "            if dress_folder.endswith(\".csv\"):\n",
    "                action_list = pd.read_csv(folder_path + dress_folder)\n",
    "            else:\n",
    "                dress_image_list = []\n",
    "                image_folder = os.path.join(folder_path, dress_folder)\n",
    "                order_image_list = os.listdir(image_folder)\n",
    "                if self.revert:\n",
    "                    order_image_list = order_image_list[::-1]\n",
    "                for file_name in order_image_list:\n",
    "                    #image_list.append(os.path.join(image_folder, file_name))\n",
    "                    single_image_path = os.path.join(image_folder, file_name)\n",
    "                    dress_image_list.append(single_image_path)\n",
    "\n",
    "                image_list.append(dress_image_list)\n",
    "        \n",
    "        return image_list, action_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_list = [] \n",
    "        for dress_image_path in self.image_list[index]:\n",
    "            #print(dress_image_path)\n",
    "            # Open image\n",
    "            img = PIL.Image.open(dress_image_path).convert(\"L\")\n",
    "            img = PIL.ImageOps.invert(img)\n",
    "            #img = resize(img,(IMAGE_SIZE,IMAGE_SIZE))\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "            \n",
    "            img_list.append(img)\n",
    "            \n",
    "        if self.transforms is None:\n",
    "            return img_list\n",
    "        \n",
    "        img_tensor = torch.stack(img_list,dim=0)\n",
    "        return img_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE), interpolation= PIL.Image.NEAREST),\n",
    "#     torchvision.transforms.RandomAffine(degrees = 90, translate = (0.2,0.2), scale = (0.6,1)),\n",
    "#     #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     torchvision.transforms.RandomVerticalFlip(),\n",
    "#     # torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = ClothImages(f_path, revert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, **kwargs):\n",
    "    # The convolutional layers (for feature extraction) use standard layers from\n",
    "    # `torch.nn`, since they do not require adaptation.\n",
    "    # See `examples/maml/model.py` for comparison.\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, **kwargs),\n",
    "        nn.BatchNorm2d(out_channels, momentum=1., track_running_stats=False),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_features, hidden_size=64):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv3x3(in_channels, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "        self.linear_aesthetics = nn.Sequential(\n",
    "            nn.Linear(hidden_size, out_features),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, params=None):\n",
    "        features = self.features(inputs)\n",
    "        #print(features.shape)\n",
    "        features = features.view((features.size(0), -1))\n",
    "        #print(features.shape)\n",
    "        scores = self.linear_aesthetics(features)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNetwork(1, 1, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUDA and torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_after = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE),interpolation = PIL.Image.NEAREST),\n",
    "    torchvision.transforms.RandomAffine(degrees = 90, translate = (0.2,0.2), scale = (0.6,1)),\n",
    "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_score(cloth_img: np.array, transform_img, model_img, use_cuda = False):\n",
    "    img = Image.fromarray(np.uint8(cloth_img * 255), 'L')\n",
    "    img_t = transform_img(img)\n",
    "    if use_cuda:\n",
    "        img_t = img_t.to(\"cuda\")\n",
    "    img_t_score = model_img(img_t.unsqueeze(0))\n",
    "    \n",
    "    return img_t_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_epoch = []\n",
    "    for i in tqdm(range(len(ci.image_list))):\n",
    "        for j in range(len(ci.image_list[i]) - 1):\n",
    "            oriImage = read_cloth_image(ci.image_list[i][j], show_image=False)\n",
    "            nextImage = read_cloth_image(ci.image_list[i][j + 1], show_image=False)\n",
    "\n",
    "            curA, curS, curImage = fold_action_max(oriImage, model, transforms_after, use_cuda=USE_CUDA)\n",
    "\n",
    "            loss = - calculate_image_score(nextImage, transforms_after, model, USE_CUDA) \\\n",
    "                    + calculate_image_score(curImage, transforms_after, model, USE_CUDA)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_epoch.append(loss.item())\n",
    "            \n",
    "            #print(loss.item())\n",
    "    print(np.mean(loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"record/fold_me_irl.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# img0 = read_cloth_image(ci.image_list[0][0])\n",
    "# img1 = read_cloth_image(ci.image_list[0][1])\n",
    "\n",
    "# oriImage = img0\n",
    "\n",
    "# curA, curS, curImage = fold_action_max(oriImage, model, transforms_after)\n",
    "\n",
    "# # calculate_image_score(img1, transforms_after, model)\n",
    "\n",
    "# # calculate_image_score(curImage, transforms_after, model)\n",
    "\n",
    "# loss = - calculate_image_score(img1, transforms_after, model) + calculate_image_score(curImage, transforms_after, model)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ConvolutionalNeuralNetwork(1, 1, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = torch.load(\"10_27.pth\")\n",
    "\n",
    "model2_dict = model2.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model2_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model2_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "model2.load_state_dict(model2_dict)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model2 = model2.cuda()\n",
    "\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5fe46c5fe34be8915165c23821cad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.039847165087444916\n",
      "-0.022954817602617875\n",
      "-0.031979949575745396\n",
      "-0.006837024146484004\n",
      "-0.09009100289808379\n",
      "-0.10022356950988372\n",
      "0.00658157544158813\n",
      "-0.2774198729958799\n",
      "0.00977611443441775\n",
      "-0.01894528501563602\n",
      "-0.0023385658860206604\n",
      "0.013186122808191512\n",
      "0.0005447806583510505\n",
      "0.00022645129097832573\n",
      "-1.644591490427653e-05\n",
      "-0.00017841822571224638\n",
      "0.00021112793021731905\n",
      "0.0002344739105966356\n",
      "0.00045321881771087646\n",
      "0.0001265952984491984\n",
      "-7.231119606229995e-05\n",
      "3.848473230997721e-05\n",
      "-0.00010244548320770264\n",
      "5.668981207741631e-05\n",
      "-0.0005213146408398946\n",
      "-0.00018046547969182333\n",
      "-0.0002911951806810167\n",
      "-0.0002224743366241455\n",
      "-0.0005182474851608276\n",
      "-0.00016880366537306044\n",
      "-0.00037504401471879747\n",
      "0.0004076436161994934\n",
      "-0.0010745301842689514\n",
      "-0.0003997774587737189\n",
      "-0.0002064224746492174\n",
      "-0.03508132261534532\n",
      "-0.06460794814241429\n",
      "0.008196556285030156\n",
      "0.007419454712362494\n",
      "-1.3572957767691049e-05\n",
      "-0.0002671883190083059\n",
      "-0.0012449729644382994\n",
      "0.002121790316272786\n",
      "-0.0008873952978723941\n",
      "0.0006268961527287805\n",
      "0.0010122399419035194\n",
      "-0.0011543868204171304\n",
      "0.001067523583009865\n",
      "0.0002566770356982791\n",
      "-0.0005332201294044757\n",
      "-0.0005734726025063234\n",
      "-0.0005662683437953496\n",
      "-0.0021406390555461664\n",
      "0.0011270218868351851\n",
      "0.0011759379291712928\n",
      "0.0018294121764483862\n",
      "-0.0003769279315343334\n",
      "-0.0029100306985330665\n",
      "0.006898676242498267\n",
      "-0.0017254614762754904\n",
      "-0.0003100996157930543\n",
      "-0.006209219786493729\n",
      "-0.025249071561524436\n",
      "0.003350875958050084\n",
      "-0.014452720820877908\n",
      "-0.028876803511391498\n",
      "-0.05727052736134889\n",
      "0.006870097260818713\n",
      "-0.0011872901684708064\n",
      "0.021495909119645756\n",
      "0.024881224665376876\n",
      "0.014055867162015703\n",
      "0.0028705663151211208\n",
      "-0.0042600300576951765\n",
      "-0.0020975942413012185\n",
      "0.00717735704448488\n",
      "0.006668081714047326\n",
      "-0.00018129580550723604\n",
      "-0.010493304994371202\n",
      "-0.02746437769383192\n",
      "-0.020515489391982555\n",
      "-0.10702615756438011\n",
      "-0.12308160430984572\n",
      "0.01912102232583695\n",
      "0.019854394098122913\n",
      "-0.05533042301734289\n",
      "-0.06756187662378782\n",
      "-0.12856139108124706\n",
      "0.04092025229086479\n",
      "-0.07699902221146557\n",
      "0.017854304997146957\n",
      "-0.06353345724831645\n",
      "-0.03823481378882813\n",
      "-0.016184315809773073\n",
      "-0.11151390181880237\n",
      "-0.0917199984703782\n",
      "-0.08101604931612706\n",
      "-0.11871860519750044\n",
      "-0.1391031101035575\n",
      "-0.07804887824588352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_epoch = []\n",
    "    for i in range(len(ci.image_list)):\n",
    "        for j in range(len(ci.image_list[i]) - 1):\n",
    "            oriImage = read_cloth_image(ci.image_list[i][j], show_image=False)\n",
    "            nextImage = read_cloth_image(ci.image_list[i][j + 1], show_image=False)\n",
    "\n",
    "            curA, curS, curImage = fold_action_max(oriImage, model2, transforms_after, use_cuda=USE_CUDA)\n",
    "\n",
    "            loss = - calculate_image_score(nextImage, transforms_after, model2, USE_CUDA) \\\n",
    "                    + calculate_image_score(curImage, transforms_after, model2, USE_CUDA)\n",
    "\n",
    "            optimizer2.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            \n",
    "            loss_epoch.append(loss.item())\n",
    "            \n",
    "            #print(loss.item())\n",
    "    print(np.mean(loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2, \"record/fold_me_irl_pre.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"record/fold_me_irl.pth\")\n",
    "model2 = torch.load(\"record/fold_me_irl_pre.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNeuralNetwork(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_aesthetics): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model2 = model2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test= torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE),interpolation = PIL.Image.NEAREST),\n",
    "    torchvision.transforms.RandomAffine(degrees = 0, translate = (0.0,0.0), scale = (0.8,0.8)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloth_folder = \"C:\\\\Users\\\\Yizhou Zhao\\\\Desktop\\\\AI\\\\validation\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(cloth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(cloth_folder):\n",
    "    if file_name.endswith(\".png\"):\n",
    "        print(\"-----------\\n\",file_name)\n",
    "        cloth_name = file_name\n",
    "\n",
    "        cloth_file = cloth_folder + cloth_name\n",
    "\n",
    "        cloth_img = read_cloth_image(cloth_file)\n",
    "\n",
    "        #plt.imshow(cloth_img, cmap=\"gray\")\n",
    "\n",
    "        img = Image.fromarray(np.uint8(cloth_img * 255) , 'L')\n",
    "\n",
    "        t3 = transforms_test(img)\n",
    "\n",
    "        print(t3.shape)\n",
    "\n",
    "        plt.imshow(t3[0].data.numpy(), cmap=\"gray\")\n",
    "\n",
    "        print(model(t3.unsqueeze(0)).item(), model2(t3.unsqueeze(0)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAtElEQVR4nO2YwQ6AIAxDmfH/fxkv6kG22QESIe1BCTTPDhIkSE79tX3AJJRQQgn9OXT3BuV855e+p+YpH4KK2myERuVB3VTeIJZUioarJVZfyrbSFYTWC4XK/egIDcmGvsayDXBSwasfXD4Qy7S4m3T0K5espBDCMo2cU7BWwzYwKbwoulGFBhZatUr5/w4gU0raCWCJnZ9QQgkllFBCCV0fGrxQVuzKWapdU88poYQSSmidDhcdCMDyAM1/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x1FE01287D48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAXUlEQVR4nO3YMQrAIAwF0KZ4/yvr1ElpidRBfH8M8iBkMYl6/Z97gQmFQqFQKBQKhUKh0GRKX4ok0e+M+7QPhUKhUCh0AZo8KA+exxvx/FXqRy2FzmbrQUGh0APQBjo6CKPl2CKKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x1FE638EEDC8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.0423]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAXElEQVR4nO3YMQrAIAwF0KZ4/yvbrYsoWnWQvj8GeRKyRCNf63NvMKFQKBQKhUKhUCgUOphUlqJ2tvtxeE77UCgUCoVCN6DzH8rRIt5dZfCeJvo1Rw8KCoX+AH0A2toGpKK8x4wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x1FE638EE988>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9185]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.9649]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAS0lEQVR4nO3YQQoAIAgEwIz+/+V6QAQFBgWzR5GBPWr0kp96wYRCoVAoFAqFQqFQ6GHaPIrV7vZx+E99KBQKhUITEv6nUCgUCn0UHYTeA6fVdLrwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x1FE638EEC08>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9955]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.9760]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(ci.image_list[index])):\n",
    "    cloth_img = read_cloth_image(ci.image_list[index][j])\n",
    "    print(calculate_image_score(cloth_img, transforms_test, model))\n",
    "    print(calculate_image_score(cloth_img, transforms_test, model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
