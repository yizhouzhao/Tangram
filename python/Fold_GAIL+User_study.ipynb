{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fold_actions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = \"data/fold_validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothImages(Dataset):\n",
    "    def __init__(self, folder_path,  transforms=None, revert=False):\n",
    "        self.revert = revert\n",
    "        self.data_root_folder = folder_path\n",
    "        self.image_list, self.action_list  = self.load_image_list(self.data_root_folder)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def load_image_list(self, folder_path):\n",
    "        image_list = []\n",
    "        action_list = None\n",
    "        for dress_folder in os.listdir(folder_path):\n",
    "            if dress_folder.endswith(\".csv\"):\n",
    "                action_list = pd.read_csv(folder_path + dress_folder)\n",
    "            else:\n",
    "                dress_image_list = []\n",
    "                image_folder = os.path.join(folder_path, dress_folder)\n",
    "                order_image_list = os.listdir(image_folder)\n",
    "                if self.revert:\n",
    "                    order_image_list = order_image_list[::-1]\n",
    "                for file_name in order_image_list:\n",
    "                    #image_list.append(os.path.join(image_folder, file_name))\n",
    "                    single_image_path = os.path.join(image_folder, file_name)\n",
    "                    dress_image_list.append(single_image_path)\n",
    "\n",
    "                image_list.append(dress_image_list)\n",
    "        \n",
    "        return image_list, action_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_list = [] \n",
    "        for dress_image_path in self.image_list[index]:\n",
    "            #print(dress_image_path)\n",
    "            # Open image\n",
    "            img = PIL.Image.open(dress_image_path).convert(\"L\")\n",
    "            img = PIL.ImageOps.invert(img)\n",
    "            #img = resize(img,(IMAGE_SIZE,IMAGE_SIZE))\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "            \n",
    "            img_list.append(img)\n",
    "            \n",
    "        if self.transforms is None:\n",
    "            return img_list\n",
    "        \n",
    "        img_tensor = torch.stack(img_list,dim=0)\n",
    "        return img_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE), interpolation= PIL.Image.NEAREST),\n",
    "#     torchvision.transforms.RandomAffine(degrees = 90, translate = (0.2,0.2), scale = (0.6,1)),\n",
    "#     #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     torchvision.transforms.RandomVerticalFlip(),\n",
    "#     # torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = ClothImages(f_path, revert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, **kwargs):\n",
    "    # The convolutional layers (for feature extraction) use standard layers from\n",
    "    # `torch.nn`, since they do not require adaptation.\n",
    "    # See `examples/maml/model.py` for comparison.\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, **kwargs),\n",
    "        nn.BatchNorm2d(out_channels, momentum=1., track_running_stats=False),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_features, hidden_size=64):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv3x3(in_channels, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "        self.linear_aesthetics = nn.Sequential(\n",
    "            nn.Linear(hidden_size, out_features),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, params=None):\n",
    "        features = self.features(inputs)\n",
    "        #print(features.shape)\n",
    "        features = features.view((features.size(0), -1))\n",
    "        #print(features.shape)\n",
    "        scores = self.linear_aesthetics(features)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNetwork(1, 1, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUDA and torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_after = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE),interpolation = PIL.Image.NEAREST),\n",
    "    torchvision.transforms.RandomAffine(degrees = 90, translate = (0.2,0.2), scale = (0.6,1)),\n",
    "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_image_score(cloth_img: np.array, transform_img, model_img, use_cuda = False):\n",
    "#     img = Image.fromarray(np.uint8(cloth_img * 255), 'L')\n",
    "#     img_t = transform_img(img)\n",
    "#     if use_cuda:\n",
    "#         img_t = img_t.to(\"cuda\")\n",
    "#     img_t_score = model_img(img_t.unsqueeze(0))\n",
    "    \n",
    "#     return img_t_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_epoch = []\n",
    "    for i in range(len(ci.image_list)):\n",
    "        for j in range(len(ci.image_list[i]) - 1):\n",
    "            oriImage = read_cloth_image(ci.image_list[i][j], show_image=False)\n",
    "            nextImage = read_cloth_image(ci.image_list[i][j + 1], show_image=False)\n",
    "\n",
    "            curA, curS, curImage = fold_action_max(oriImage, model, transforms_after, use_cuda=USE_CUDA)\n",
    "            \n",
    "            # print(curImage.shape)\n",
    "            expert_score = calculate_image_score(nextImage, transforms_after, model, USE_CUDA)\n",
    "            sample_score = calculate_image_score(curImage, transforms_after, model, USE_CUDA)\n",
    "            \n",
    "            # print(expert_score)\n",
    "            # print(sample_score)\n",
    "            \n",
    "            loss = - torch.log(0.0001 + expert_score) \\\n",
    "                    - torch.log(1.001 - sample_score)\n",
    "            \n",
    "            #loss = - expert_score + sample_score\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_epoch.append(loss.item())\n",
    "            \n",
    "            #print(loss.item())\n",
    "    print(np.mean(loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"record/fold_gail.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# img0 = read_cloth_image(ci.image_list[0][0])\n",
    "# img1 = read_cloth_image(ci.image_list[0][1])\n",
    "\n",
    "# oriImage = img0\n",
    "\n",
    "# curA, curS, curImage = fold_action_max(oriImage, model, transforms_after)\n",
    "\n",
    "# # calculate_image_score(img1, transforms_after, model)\n",
    "\n",
    "# # calculate_image_score(curImage, transforms_after, model)\n",
    "\n",
    "# loss = - calculate_image_score(img1, transforms_after, model) + calculate_image_score(curImage, transforms_after, model)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ConvolutionalNeuralNetwork(1, 1, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = torch.load(\"10_29.pth\")\n",
    "\n",
    "model2_dict = model2.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model2_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model2_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "model2.load_state_dict(model2_dict)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model2 = model2.cuda()\n",
    "\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_epoch = []\n",
    "    for i in tqdm(range(len(ci.image_list))):\n",
    "        for j in range(len(ci.image_list[i]) - 1):\n",
    "            oriImage = read_cloth_image(ci.image_list[i][j], show_image=False)\n",
    "            nextImage = read_cloth_image(ci.image_list[i][j + 1], show_image=False)\n",
    "\n",
    "            curA, curS, curImage = fold_action_max(oriImage, model2, transforms_after, use_cuda=USE_CUDA)\n",
    "\n",
    "            # print(curImage.shape)\n",
    "            expert_score = calculate_image_score(nextImage, transforms_after, model2, USE_CUDA)\n",
    "            sample_score = calculate_image_score(curImage, transforms_after, model2, USE_CUDA)\n",
    "            \n",
    "            # print(expert_score)\n",
    "            # print(sample_score)\n",
    "            \n",
    "            loss = - torch.log(0.0001 + expert_score) \\\n",
    "                    - torch.log(1.001 - sample_score)\n",
    "\n",
    "            optimizer2.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            \n",
    "            loss_epoch.append(loss.item())\n",
    "            \n",
    "            #print(loss.item())\n",
    "    print(np.mean(loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model2, \"record/fold_gail_pre.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"record/fold_gail.pth\")\n",
    "model2 = torch.load(\"record/fold_gail_pre.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model2 = model2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test = transforms_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test= torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE),interpolation = PIL.Image.NEAREST),\n",
    "    torchvision.transforms.RandomAffine(degrees = 0, translate = (0.0,0.0), scale = (0.8,0.8)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloth_folder = \"C:\\\\Users\\\\Yizhou Zhao\\\\Desktop\\\\AI\\\\validation\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(cloth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(cloth_folder):\n",
    "    if file_name.endswith(\".png\"):\n",
    "        print(\"-----------\\n\",file_name)\n",
    "        cloth_name = file_name\n",
    "\n",
    "        cloth_file = cloth_folder + cloth_name\n",
    "\n",
    "        cloth_img = read_cloth_image(cloth_file)\n",
    "\n",
    "        #plt.imshow(cloth_img, cmap=\"gray\")\n",
    "\n",
    "        img = Image.fromarray(np.uint8(cloth_img * 255) , 'L')\n",
    "\n",
    "        t3 = transforms_test(img)\n",
    "\n",
    "        print(t3.shape)\n",
    "\n",
    "        plt.imshow(t3[0].data.numpy(), cmap=\"gray\")\n",
    "\n",
    "        print(model(t3.unsqueeze(0)).item(), model2(t3.unsqueeze(0)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(6):\n",
    "    for jj in [0,1]:\n",
    "        if jj == 0:\n",
    "            q_model = model\n",
    "        else:\n",
    "            q_model = model2\n",
    "\n",
    "        cloth_img = read_cloth_image(ci.image_list[index][0])\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        save_fold_folder = \"data/user_study_step_1/\" + \"exampleB\" + str(index) + str(jj)\n",
    "\n",
    "        if not os.path.exists(save_fold_folder):\n",
    "            os.mkdir(save_fold_folder)\n",
    "\n",
    "        while step < 6:\n",
    "            cloth_img_I = Image.fromarray(np.uint8(cloth_img * 255) , 'L')\n",
    "            cloth_img_I = PIL.ImageOps.invert(cloth_img_I)\n",
    "            display(cloth_img_I)\n",
    "            cloth_img_I.save(save_fold_folder + \"/\" + str(step) + \".png\")\n",
    "            score, path_image_list = fold_action_max_beam_search(cloth_img, q_model, transforms_test, steps = 1)\n",
    "        #     for i in range(len(path_image_list)):\n",
    "        #         plt.imshow(path_image_list[i],cmap=\"gray\")\n",
    "        #         plt.show()\n",
    "            if np.sum((cloth_img - path_image_list[-1])**2) > 5:\n",
    "                cloth_img = path_image_list[-1]\n",
    "            elif np.sum((cloth_img - path_image_list[0])**2) > 5:\n",
    "                cloth_img = path_image_list[0]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(len(ci.image_list[index])):\n",
    "#     cloth_img = read_cloth_image(ci.image_list[index][j])\n",
    "#     print(calculate_image_score(cloth_img, transforms_test, model))\n",
    "#     print(calculate_image_score(cloth_img, transforms_test, model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cloth_img = read_cloth_image(ci.image_list[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fold_folder = \"data/user_study/\" + \"example0b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_fold_folder):\n",
    "    os.mkdir(save_fold_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while step < 6:\n",
    "    cloth_img_I = Image.fromarray(np.uint8(cloth_img * 255) , 'L')\n",
    "    cloth_img_I = PIL.ImageOps.invert(cloth_img_I)\n",
    "    display(cloth_img_I)\n",
    "    cloth_img_I.save(save_fold_folder + \"/\" + str(step) + \".png\")\n",
    "    score, path_image_list = fold_action_max_beam_search(cloth_img, q_model, transforms_test, steps = 2)\n",
    "#     for i in range(len(path_image_list)):\n",
    "#         plt.imshow(path_image_list[i],cmap=\"gray\")\n",
    "#         plt.show()\n",
    "    if np.sum((cloth_img - path_image_list[-1])**2) > 5:\n",
    "        cloth_img = path_image_list[-1]\n",
    "    elif np.sum((cloth_img - path_image_list[0])**2) > 5:\n",
    "        cloth_img = path_image_list[0]\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    step += 1\n",
    "    \n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cloth_img_I = Image.fromarray(np.uint8(cloth_img * 255) , 'L')\n",
    "cloth_img_I = PIL.ImageOps.invert(cloth_img_I)\n",
    "display(cloth_img_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloth_img_I.save(save_fold_folder + \"/\" + str(step) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, path_image_list = fold_action_max_beam_search(cloth_img, model, transforms_test, steps = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(path_image_list)):\n",
    "    plt.imshow(path_image_list[i],cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.sum((cloth_img - path_image_list[-1])**2) > 5:\n",
    "    cloth_img = path_image_list[-1]\n",
    "elif np.sum((cloth_img - path_image_list[0])**2) > 5:\n",
    "    cloth_img = path_image_list[0]\n",
    "else:\n",
    "    print(\"no renew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloth_img = path_image_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
