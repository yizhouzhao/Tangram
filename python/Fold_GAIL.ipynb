{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fold_actions import point_line_symmetry, matrix_line_symmetry, read_cloth_image, fold_action_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = \"data/fold/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothImages(Dataset):\n",
    "    def __init__(self, folder_path,  transforms=None, revert=False):\n",
    "        self.revert = revert\n",
    "        self.data_root_folder = folder_path\n",
    "        self.image_list, self.action_list  = self.load_image_list(self.data_root_folder)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def load_image_list(self, folder_path):\n",
    "        image_list = []\n",
    "        action_list = None\n",
    "        for dress_folder in os.listdir(folder_path):\n",
    "            if dress_folder.endswith(\".csv\"):\n",
    "                action_list = pd.read_csv(folder_path + dress_folder)\n",
    "            else:\n",
    "                dress_image_list = []\n",
    "                image_folder = os.path.join(folder_path, dress_folder)\n",
    "                order_image_list = os.listdir(image_folder)\n",
    "                if self.revert:\n",
    "                    order_image_list = order_image_list[::-1]\n",
    "                for file_name in order_image_list:\n",
    "                    #image_list.append(os.path.join(image_folder, file_name))\n",
    "                    single_image_path = os.path.join(image_folder, file_name)\n",
    "                    dress_image_list.append(single_image_path)\n",
    "\n",
    "                image_list.append(dress_image_list)\n",
    "        \n",
    "        return image_list, action_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_list = [] \n",
    "        for dress_image_path in self.image_list[index]:\n",
    "            #print(dress_image_path)\n",
    "            # Open image\n",
    "            img = PIL.Image.open(dress_image_path).convert(\"L\")\n",
    "            img = PIL.ImageOps.invert(img)\n",
    "            #img = resize(img,(IMAGE_SIZE,IMAGE_SIZE))\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "            \n",
    "            img_list.append(img)\n",
    "            \n",
    "        if self.transforms is None:\n",
    "            return img_list\n",
    "        \n",
    "        img_tensor = torch.stack(img_list,dim=0)\n",
    "        return img_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE), interpolation= PIL.Image.NEAREST),\n",
    "#     torchvision.transforms.RandomAffine(degrees = 90, translate = (0.2,0.2), scale = (0.6,1)),\n",
    "#     #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     torchvision.transforms.RandomVerticalFlip(),\n",
    "#     # torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = ClothImages(f_path, revert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, **kwargs):\n",
    "    # The convolutional layers (for feature extraction) use standard layers from\n",
    "    # `torch.nn`, since they do not require adaptation.\n",
    "    # See `examples/maml/model.py` for comparison.\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, **kwargs),\n",
    "        nn.BatchNorm2d(out_channels, momentum=1., track_running_stats=False),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_features, hidden_size=64):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv3x3(in_channels, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "        self.linear_aesthetics = nn.Sequential(\n",
    "            nn.Linear(hidden_size, out_features),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, params=None):\n",
    "        features = self.features(inputs)\n",
    "        #print(features.shape)\n",
    "        features = features.view((features.size(0), -1))\n",
    "        #print(features.shape)\n",
    "        scores = self.linear_aesthetics(features)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNetwork(1, 1, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUDA and torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_after = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE),interpolation = PIL.Image.NEAREST),\n",
    "    torchvision.transforms.RandomAffine(degrees = 90, translate = (0.2,0.2), scale = (0.6,1)),\n",
    "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_score(cloth_img: np.array, transform_img, model_img, use_cuda = False):\n",
    "    img = Image.fromarray(np.uint8(cloth_img * 255), 'L')\n",
    "    img_t = transforms_after(img)\n",
    "    if use_cuda:\n",
    "        img_t = img_t.to(\"cuda\")\n",
    "    img_t_score = model_img(img_t.unsqueeze(0))\n",
    "    \n",
    "    return img_t_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329f66ae3bea406fb55499401b6b0a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4f426fe3acad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mnextImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_cloth_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mci\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mcurA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfold_action_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms_after\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUSE_CUDA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;31m# print(curImage.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\researches\\Tangram\\python\\fold_actions.py\u001b[0m in \u001b[0;36mfold_action_max\u001b[1;34m(cloth, value_model, transforms_after, method, use_cuda)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mcloth_folded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix_line_symmetry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[0mimg_folded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloth_folded\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mimg_folded_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms_after\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_folded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\researches\\Tangram\\python\\fold_actions.py\u001b[0m in \u001b[0;36mmatrix_line_symmetry\u001b[1;34m(matrix, a, b, c)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mfold_direction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mnew_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_epoch = []\n",
    "    for i in range(len(ci.image_list)):\n",
    "        for j in range(len(ci.image_list[i]) - 1):\n",
    "            oriImage = read_cloth_image(ci.image_list[i][j], show_image=False)\n",
    "            nextImage = read_cloth_image(ci.image_list[i][j + 1], show_image=False)\n",
    "\n",
    "            curA, curS, curImage = fold_action_max(oriImage, model, transforms_after, use_cuda=USE_CUDA)\n",
    "            \n",
    "            # print(curImage.shape)\n",
    "            expert_score = calculate_image_score(nextImage, transforms_after, model, USE_CUDA)\n",
    "            sample_score = calculate_image_score(curImage, transforms_after, model, USE_CUDA)\n",
    "            \n",
    "            # print(expert_score)\n",
    "            # print(sample_score)\n",
    "            \n",
    "            loss = - torch.log(0.0001 + expert_score) \\\n",
    "                    - torch.log(1.001 - sample_score)\n",
    "            \n",
    "            #loss = - expert_score + sample_score\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_epoch.append(loss.item())\n",
    "            \n",
    "            #print(loss.item())\n",
    "    print(np.mean(loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"record/fold_gail.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# img0 = read_cloth_image(ci.image_list[0][0])\n",
    "# img1 = read_cloth_image(ci.image_list[0][1])\n",
    "\n",
    "# oriImage = img0\n",
    "\n",
    "# curA, curS, curImage = fold_action_max(oriImage, model, transforms_after)\n",
    "\n",
    "# # calculate_image_score(img1, transforms_after, model)\n",
    "\n",
    "# # calculate_image_score(curImage, transforms_after, model)\n",
    "\n",
    "# loss = - calculate_image_score(img1, transforms_after, model) + calculate_image_score(curImage, transforms_after, model)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ConvolutionalNeuralNetwork(1, 1, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = torch.load(\"10_29.pth\")\n",
    "\n",
    "model2_dict = model2.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model2_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model2_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "model2.load_state_dict(model2_dict)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model2 = model2.cuda()\n",
    "\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_epoch = []\n",
    "    for i in tqdm(range(len(ci.image_list))):\n",
    "        for j in range(len(ci.image_list[i]) - 1):\n",
    "            oriImage = read_cloth_image(ci.image_list[i][j], show_image=False)\n",
    "            nextImage = read_cloth_image(ci.image_list[i][j + 1], show_image=False)\n",
    "\n",
    "            curA, curS, curImage = fold_action_max(oriImage, model2, transforms_after, use_cuda=USE_CUDA)\n",
    "\n",
    "            # print(curImage.shape)\n",
    "            expert_score = calculate_image_score(nextImage, transforms_after, model2, USE_CUDA)\n",
    "            sample_score = calculate_image_score(curImage, transforms_after, model2, USE_CUDA)\n",
    "            \n",
    "            # print(expert_score)\n",
    "            # print(sample_score)\n",
    "            \n",
    "            loss = - torch.log(0.0001 + expert_score) \\\n",
    "                    - torch.log(1.001 - sample_score)\n",
    "\n",
    "            optimizer2.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            \n",
    "            loss_epoch.append(loss.item())\n",
    "            \n",
    "            #print(loss.item())\n",
    "    print(np.mean(loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model2, \"record/fold_gail_pre.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"record/fold_gail.pth\")\n",
    "model2 = torch.load(\"record/fold_gail_pre.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNeuralNetwork(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_aesthetics): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model2 = model2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test = transforms_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test= torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMAGE_SIZE,IMAGE_SIZE),interpolation = PIL.Image.NEAREST),\n",
    "    torchvision.transforms.RandomAffine(degrees = 0, translate = (0.0,0.0), scale = (0.8,0.8)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloth_folder = \"C:\\\\Users\\\\Yizhou Zhao\\\\Desktop\\\\AI\\\\validation\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(cloth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(cloth_folder):\n",
    "    if file_name.endswith(\".png\"):\n",
    "        print(\"-----------\\n\",file_name)\n",
    "        cloth_name = file_name\n",
    "\n",
    "        cloth_file = cloth_folder + cloth_name\n",
    "\n",
    "        cloth_img = read_cloth_image(cloth_file)\n",
    "\n",
    "        #plt.imshow(cloth_img, cmap=\"gray\")\n",
    "\n",
    "        img = Image.fromarray(np.uint8(cloth_img * 255) , 'L')\n",
    "\n",
    "        t3 = transforms_test(img)\n",
    "\n",
    "        print(t3.shape)\n",
    "\n",
    "        plt.imshow(t3[0].data.numpy(), cmap=\"gray\")\n",
    "\n",
    "        print(model(t3.unsqueeze(0)).item(), model2(t3.unsqueeze(0)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAz0lEQVR4nO2YwQ6DMAxDm2n//8vdYUITw24dGNKiOhcgcp6atgjq6O338biBaaih60KfM0G01scJUDMUxPvSx6kUND63nadS0Ng/dpxKQQOnv4IVw7yGpFi4pXQmlkJo4mugj/RqGApCXiksrN6+oYaWgIrvKZGVb9/QlaHiPwqRlW/fUEMLQOWTBBZWb39t6C3HyKtRG5qYUiyu3X7KUUZiYsyo00qq2bAULK3lve6xR19qUKnYcsBBO23LbZSDgThbyhn0VNTe/IYa+m/QF82PJpVagm9lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x15FC8456D48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6098]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.4707]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAArElEQVR4nO3Y0QqAMAgFUI3+/5frpYchbmF3iy5dXyqoAzalph82P7YFplCh/0X3uxvczKqt7MMH/DoW1RHqzXmJ7aMergtsD41kic0XKiMLkZYUaOYo+jXg6Sh2FFwp9vSFCqVAsT6lT1/on1HsH4U+faFCCdAVuxM0hL6ELtlGosGNoq+UPH14opxPJtZseDF2MOwKRTBhLhXdSRO0hp07QHwY3MUvVOjX0BO0phmZXB135QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x15FB771A448>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3979]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.4558]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAiUlEQVR4nO3YQQ6AIAxEUWu8/5VxY2I0LVJHFo1/liQ+2ioLtLZ8n3WCCQoKCgoKCgoKCgoKCprMFqybszZ8OQwq9czx1JkpKGhdVDul5dv/Nyq+/Ortz0DVkRZvX/6j7Faqqn77TWMtfPz+DST2idErmyq9h55schp99HCzE35E36T2iQIFFbIDVKcPnwz2xkoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x15FC8457088>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4519]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5131]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAhUlEQVR4nO3Y0QqAIAyF4Ra9/yvbTRDJtNbJZPDvUvBzm3ozK8v3sQ4wQUFBQUFBQUFBQUFBQUFB56Kbv2ze4uN5Q57ys6PuPamoGKA/oeLlZy9/BKq2NHn58kTZzVRV/fKLxlpze/0GAue00SsbSr2HnmywG330cKMdvkXfRO4fBQoqxA7vTw+eFwnRQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x15FC8456D48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4220]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5812]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(ci.image_list[index])):\n",
    "    cloth_img = read_cloth_image(ci.image_list[index][j])\n",
    "    print(calculate_image_score(cloth_img, transforms_test, model))\n",
    "    print(calculate_image_score(cloth_img, transforms_test, model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
